{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert Approach: Predicting Farmer Adoption with CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "Loading necessary libraries and datasets. The ID column is immediately set as the index for alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (5548, 18)\n",
      "Test data shape: (2387, 15)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from collections import Counter\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('Train.csv')\n",
    "test_df = pd.read_csv('Test.csv')\n",
    "sample_submission_df = pd.read_csv('SampleSubmission.csv')\n",
    "\n",
    "# Set Index\n",
    "train_df.set_index('ID', inplace=True)\n",
    "test_df.set_index('ID', inplace=True)\n",
    "\n",
    "# Combine for universal processing\n",
    "combined_df = pd.concat([train_df.drop(['adopted_within_07_days', 'adopted_within_90_days', 'adopted_within_120_days'], axis=1), test_df], axis=0)\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Feature Engineering\n",
    "This section focuses on creating high-impact features through temporal analysis, behavioral parsing, and creating interaction and aggregation features to capture deeper patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'farming_experience'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/core/indexes/base.py:3641\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3640\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3642\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:168\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:197\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7668\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7676\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'farming_experience'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     36\u001b[39m         df[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_experience_mean\u001b[39m\u001b[33m'\u001b[39m] = df.groupby(geo)[\u001b[33m'\u001b[39m\u001b[33mfarming_experience\u001b[39m\u001b[33m'\u001b[39m].transform(\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m combined_df_featured = \u001b[43mexpert_feature_engineer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mAdvanced feature engineering complete.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape after FE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_df_featured.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mexpert_feature_engineer\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Interaction and Derived Features\u001b[39;00m\n\u001b[32m     23\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtraining_intensity\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mnum_repeat_trainings\u001b[39m\u001b[33m'\u001b[39m] / (df[\u001b[33m'\u001b[39m\u001b[33mnum_total_trainings\u001b[39m\u001b[33m'\u001b[39m] + \u001b[32m1e-6\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mexperience_per_age\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfarming_experience\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m / (df[\u001b[33m'\u001b[39m\u001b[33mage\u001b[39m\u001b[33m'\u001b[39m] + \u001b[32m1e-6\u001b[39m)\n\u001b[32m     25\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mfarm_size_per_person\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mfarm_size\u001b[39m\u001b[33m'\u001b[39m] / (df[\u001b[33m'\u001b[39m\u001b[33mgroup_size\u001b[39m\u001b[33m'\u001b[39m] + \u001b[32m1e-6\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Handling Missing Values\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/core/frame.py:4378\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4378\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4380\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/core/indexes/base.py:3648\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3644\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3645\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3646\u001b[39m     ):\n\u001b[32m   3647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3650\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3651\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3652\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3653\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'farming_experience'"
     ]
    }
   ],
   "source": [
    "def expert_feature_engineer(df):\n",
    "    # Temporal Engineering\n",
    "    df['first_training_date'] = pd.to_datetime(df['first_training_date'])\n",
    "    df['month'] = df['first_training_date'].dt.month\n",
    "    df['year'] = df['first_training_date'].dt.year\n",
    "    df['dayofyear'] = df['first_training_date'].dt.dayofyear\n",
    "    df['weekofyear'] = df['first_training_date'].dt.isocalendar().week.astype(int)\n",
    "    df['dayofweek'] = df['first_training_date'].dt.dayofweek\n",
    "    df.drop('first_training_date', axis=1, inplace=True)\n",
    "\n",
    "    # Behavioral Engineering from topics_list\n",
    "    df['topics_list'] = df['topics_list'].apply(ast.literal_eval)\n",
    "    df['topic_count'] = df['topics_list'].apply(len)\n",
    "    \n",
    "    # Extracting top topics to create binary flags\n",
    "    all_topics = [topic for sublist in df['topics_list'] for topic in sublist]\n",
    "    top_10_topics = [topic for topic, count in Counter(all_topics).most_common(10)]\n",
    "    for topic in top_10_topics:\n",
    "        df[f'topic_{topic.replace(\" \", \"_\")}'] = df['topics_list'].apply(lambda x: 1 if topic in x else 0)\n",
    "    df.drop('topics_list', axis=1, inplace=True)\n",
    "\n",
    "    # Interaction and Derived Features\n",
    "    df['training_intensity'] = df['num_repeat_trainings'] / (df['num_total_trainings'] + 1e-6)\n",
    "    df['experience_per_age'] = df['farming_experience'] / (df['age'] + 1e-6)\n",
    "    df['farm_size_per_person'] = df['farm_size'] / (df['group_size'] + 1e-6)\n",
    "\n",
    "    # Handling Missing Values\n",
    "    df['days_to_second_training_missing'] = df['days_to_second_training'].isnull().astype(int)\n",
    "    df['days_to_second_training'].fillna(-999, inplace=True)\n",
    "\n",
    "    # Aggregation Features (e.g., stats per county)\n",
    "    geo_features = ['county', 'subcounty', 'ward']\n",
    "    for geo in geo_features:\n",
    "        df[f'{geo}_farm_size_mean'] = df.groupby(geo)['farm_size'].transform('mean')\n",
    "        df[f'{geo}_age_mean'] = df.groupby(geo)['age'].transform('mean')\n",
    "        df[f'{geo}_experience_mean'] = df.groupby(geo)['farming_experience'].transform('mean')\n",
    "        \n",
    "    return df\n",
    "\n",
    "combined_df_featured = expert_feature_engineer(combined_df.copy())\n",
    "\n",
    "print('Advanced feature engineering complete.')\n",
    "print(f\"Shape after FE: {combined_df_featured.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling with CatBoost\n",
    "We use CatBoost, which is excellent for handling categorical features natively. A `MultiOutputClassifier` is used to manage the three target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train and test sets\n",
    "X = combined_df_featured.loc[train_df.index]\n",
    "X_test = combined_df_featured.loc[test_df.index]\n",
    "y = train_df[['adopted_within_07_days', 'adopted_within_90_days', 'adopted_within_120_days']]\n",
    "\n",
    "# Identify categorical features for CatBoost\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "X[categorical_features] = X[categorical_features].astype(str)\n",
    "X_test[categorical_features] = X_test[categorical_features].astype(str)\n",
    "\n",
    "# Align columns before training\n",
    "X_test = X_test[X.columns]\n",
    "\n",
    "# Validation Strategy\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Definition\n",
    "catboost_params = {\n",
    "    'iterations': 1500,\n",
    "    'learning_rate': 0.03,\n",
    "    'depth': 7,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'loss_function': 'Logloss',\n",
    "    'eval_metric': 'Logloss',\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0, # Set to 100 for verbose training\n",
    "    'cat_features': categorical_features\n",
    "}\n",
    "\n",
    "base_estimator = CatBoostClassifier(**catboost_params)\n",
    "model = MultiOutputClassifier(estimator=base_estimator)\n",
    "\n",
    "# Training\n",
    "print('Starting CatBoost model training...')\n",
    "model.fit(X_train, y_train)\n",
    "print('Model training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Submission Generation\n",
    "Generating predictions on the test set and formatting the output to match the `SampleSubmission.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating predictions on the test set...')\n",
    "test_probabilities = model.predict_proba(X_test)\n",
    "\n",
    "# Extract probabilities for each target\n",
    "prob_07 = test_probabilities[0][:, 1]\n",
    "prob_90 = test_probabilities[1][:, 1]\n",
    "prob_120 = test_probabilities[2][:, 1]\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame(index=X_test.index)\n",
    "submission_df['Target_07_AUC'] = prob_07\n",
    "submission_df['Target_90_AUC'] = prob_90\n",
    "submission_df['Target_120_AUC'] = prob_120\n",
    "submission_df['Target_07_LogLoss'] = prob_07\n",
    "submission_df['Target_90_LogLoss'] = prob_90\n",
    "submission_df['Target_120_LogLoss'] = prob_120\n",
    "\n",
    "# Ensure column order matches sample submission\n",
    "submission_df = submission_df[sample_submission_df.columns[1:]]\n",
    "\n",
    "submission_df.to_csv('catboost_submission.csv')\n",
    "\n",
    "print('Submission file `catboost_submission.csv` created successfully!')\n",
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
